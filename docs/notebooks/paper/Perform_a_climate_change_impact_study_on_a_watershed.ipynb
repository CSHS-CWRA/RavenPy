{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a climate change impact study on the hydrology of a watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrological models typically need geographical information about watersheds being simulated: latitude and longitude, area, mean altitude, land-use, etc. This notebook shows how to obtain this information using remote services that are made available for users in PAVICS-Hydro. These services connect to a digital elevation model (DEM) and a land-use data set to extract relevant information.\n",
    "\n",
    "The DEM used in the following is the [EarthEnv-DEM90](https://www.earthenv.org/DEM), while the land-use dataset is the [North American Land Change Monitoring System](http://www.cec.org/north-american-environmental-atlas/land-cover-30m-2015-landsat-and-rapideye/). Other data sources could be used, given their availability through the Web Coverage Service (WCS) protocol.\n",
    "\n",
    "Since these computations happen on a specific Geoserver hosted in PAVICS, we need to establish a connection to that service. While the steps are a bit more complex, the good news is that you only need to change a few items in this notebook to taylor results to your needs.\n",
    "\n",
    "We will also setup a hydrological model, calibrate it, and use it in evaluating the impacts of climate change on the hydrology of a catchment. We will be using the Mistassini river as the test-case for this example, but you can substitute the data for any catchment of your liking. We provide:\n",
    "\n",
    "1- Streamflow observations (Water Survey Canada station 02RD003)\n",
    "\n",
    "2- Watershed boundaries in the form of shapefiles (all shape files .shp, .shx, .prj, .dbf, etc. zipped into a single file. The platform will detect and unzip the file to extract the required data)\n",
    "\n",
    "\n",
    "The rest will be done by PAVICS-Hydro, including getting meteorological information from our ERA5 reanalysis database and climate change model data from CMIP hosted by PanGEO.\n",
    "\n",
    "## Software setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages for this notebook.\n",
    "Note that since the notebook includes the entire process from data collection to climate change impact studies, there are a lot more packages required than usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to import a few packages required to do the work:\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# Basic system packages:\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Packages for data extraction on remote servers/filesystems:\n",
    "import gcsfs\n",
    "import geopandas as gpd\n",
    "\n",
    "# Packages for geographic processing:\n",
    "import intake\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Packages related to ravenpy and hydrological modelling:\n",
    "import spotpy\n",
    "import xarray as xr\n",
    "\n",
    "# Packages required for data processing:\n",
    "import xclim\n",
    "import xsdba\n",
    "from birdy import WPSClient\n",
    "from clisops.core import average, subset\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from ravenpy import Emulator\n",
    "from ravenpy.config import commands as rc\n",
    "from ravenpy.config.emulators import GR4JCN\n",
    "\n",
    "# Utility that simplifies fetching and caching test data hosted on GitHub\n",
    "from ravenpy.testing.utils import yangtze\n",
    "from ravenpy.utilities.calibration import SpotSetup\n",
    "\n",
    "yangtze = yangtze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare some boilerplate items that will be required later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The platform provides lots of user warnings and information points. We will disable them for now.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# This is the URL of the Geoserver that will perform the computations for us.\n",
    "url = os.environ.get(\n",
    "    \"WPS_URL\", \"https://pavics.ouranos.ca/twitcher/ows/proxy/raven/wps\"\n",
    ")\n",
    "\n",
    "# Connect to the PAVICS-Hydro Raven WPS server to get the geospatial data from GeoServer.\n",
    "wps = WPSClient(url)\n",
    "\n",
    "# Make a temporary path where the data will be stored and used by Raven.\n",
    "tmp = Path(tempfile.mkdtemp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets that will be required\n",
    "This includes observed streamflow for the catchment of interest as well as the polygon/contour of that watershed. These could be gathered from CANOPEX, HYSETS or other databases, but we provide an example for user convenience here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Name of the watershed boundaries file that is uploaded to the server.\n",
    "# Note that this file contains the .shx, .shp and other associated files for shapefiles, all zipped into one file.\n",
    "# It will also be used later for extracting meteorological data.\n",
    "basin_contour = yangtze.fetch(\"paper/shapefile_basin_574_HYSETS.zip\")\n",
    "\n",
    "# This file is an extraction of streamflow for catchment 574 in HYSETS.\n",
    "# Weather data will be gathered later from the ERA5 database, but could also be taken directly from HYSETS.\n",
    "# This is to show how the process could be linked together for your own applications using ERA5 data.\n",
    "streamflow_file = yangtze.fetch(\"paper/Qobs_574_HYSETS.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other user inputs of interest\n",
    "We can also specify some information such as periods of interest for reference and future periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reference period that will be used for ERA5 and climate model data for the reference period.\n",
    "# Here let's focus on a 10-year period to keep running times lower.\n",
    "reference_start_day = dt.datetime(1980, 12, 31)\n",
    "reference_end_day = dt.datetime(1990, 12, 31)\n",
    "# Notice we are using one day before the desired period of 1981-01-01 to 1990-12-31.\n",
    "# This is to account for any UTC shifts that might require getting data in a previous or later time.\n",
    "\n",
    "# Same process for the future period, 100 years later.\n",
    "future_start_day = dt.datetime(2080, 12, 31)\n",
    "future_end_day = dt.datetime(2090, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Geographic processing of watershed attributes</span>\n",
    "\n",
    "Here we will use a set of tools to extract watershed properties that can be used for various applications. Not all variables we extract here are required for the hydrological modelling, but could be used for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare a plot of the catchment to see what we are working with.\n",
    "df = gpd.read_file(basin_contour)\n",
    "display(df)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic watershed properties\n",
    "\n",
    "Now that we have delineated a watershed, lets find the zonal statistics and other properties using the `shape_properties` process. This process requires a `shape` argument defining the watershed contour, the exterior polygon.\n",
    "\n",
    "Once the process has completed, we extract the data from the response, as follows. Note that you do not need to change anything here. The code will work and return the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_resp = wps.shape_properties(shape=basin_contour)\n",
    "\n",
    "[\n",
    "    properties,\n",
    "] = shape_resp.get(asobj=True)\n",
    "prop = properties[0]\n",
    "display(prop)\n",
    "\n",
    "area = prop[\"area\"] / 1000000.0\n",
    "longitude = prop[\"centroid\"][0]\n",
    "latitude = prop[\"centroid\"][1]\n",
    "gravelius = prop[\"gravelius\"]\n",
    "perimeter = prop[\"perimeter\"]\n",
    "\n",
    "shape_info = {\n",
    "    \"area\": area,\n",
    "    \"longitude\": longitude,\n",
    "    \"latitude\": latitude,\n",
    "    \"gravelius\": gravelius,\n",
    "    \"perimeter\": perimeter,\n",
    "}\n",
    "display(shape_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these properties are a mix of the properties of the original file where the shape is stored, and properties computed by the process (area, centroid, perimeter and gravelius). Note also that the computed area is in m², while the \"SUB_AREA\" property is in km², and that there are slight differences between the two values due to the precision of HydroSHEDS and the delineation algorithm.\n",
    "\n",
    "### Land-use information\n",
    "\n",
    "Now we extract the land-use properties of the watershed using the `nalcms_zonal_stats` process. As mentioned, it uses a dataset from the [North American Land Change Monitoring System](http://www.cec.org/north-american-environmental-atlas), and retrieve properties over the given region.\n",
    "\n",
    "With the `nalcms_zonal_stats_raster` process, we also return the grid with variable accessors (`gdal`, `rasterio`, or `rioxarray`) depending on what libraries are available in our runtime environment (The following examples show `rioxarray`-like access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_resp = wps.nalcms_zonal_stats_raster(\n",
    "    shape=basin_contour, select_all_touching=True, band=1, simple_categories=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will get the raster data and compute statistics on it. It is also possible to download the extracted raseter offline (please see the tutorial for the steps on how to do this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features, statistics, grid0 = stats_resp.get(asobj=True)\n",
    "lu = statistics[0]\n",
    "total = sum(lu.values())\n",
    "\n",
    "land_use = {k: (v / total) for (k, v) in lu.items()}\n",
    "display(\"Land use ratios\", land_use)\n",
    "\n",
    "land_use_pct = {k: f\"{np.round(v/total*100, 2)} %\" for (k, v) in lu.items()}\n",
    "display(\"Land use percentages\", land_use_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terrain information from the DEM\n",
    "\n",
    "Here we collect terrain data, such as elevation, slope and aspect, from the DEM. We will do this using the `terrain_analysis` WPS service, which by default uses DEM data from [EarthEnv-DEM90](https://www.earthenv.org/DEM).\n",
    "\n",
    "Note here that while the feature outline is defined above in terms of geographic coordinates (latitude, longitude), the DEM is projected onto a 2D cartesian coordinate system (here NAD83, the Canada Atlas Lambert projection). This is necessary to perform slope calculations. For more information on this, see: https://en.wikipedia.org/wiki/Map_projection\n",
    "\n",
    "The DEM data returned in the process response here shows `rioxarray`-like access but using the URLs we can open the files however we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terrain_resp = wps.terrain_analysis(\n",
    "    shape=basin_contour, select_all_touching=True, projected_crs=3978\n",
    ")\n",
    "\n",
    "properties, dem0 = terrain_resp.get(asobj=True)\n",
    "\n",
    "elevation = properties[0][\"elevation\"]\n",
    "slope = properties[0][\"slope\"]\n",
    "aspect = properties[0][\"aspect\"]\n",
    "\n",
    "terrain = {\"elevation\": elevation, \"slope\": slope, \"aspect\": aspect}\n",
    "display(terrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "A synthesis of all watershed properties can be created by merging the various dictionaries created. This allows users to easily access any of these values, and to provide them to a Raven model as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_properties = {**shape_info, **land_use, **terrain}\n",
    "display(all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Getting meteorological and climate data</span>\n",
    "\n",
    "Now that we have all the geographic information for our watershed, we can get the input meteorological data required to calibrate and run the model, as well as climate model data that will be used to perform a climate change impact study.\n",
    "\n",
    "We start by using an in-house solution that keeps updated ERA5 reanalysis datasets available with little to no wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ERA5 data from the Wasabi/Amazon S3 server.\n",
    "catalog_name = \"https://raw.githubusercontent.com/hydrocloudservices/catalogs/main/catalogs/atmosphere.yaml\"\n",
    "cat = intake.open_catalog(catalog_name)\n",
    "ds = cat.era5_reanalysis_single_levels.to_dask()\n",
    "\n",
    "# Set the date to the midnight of the given day.\n",
    "ds = ds.assign_coords(time=ds.time.dt.floor(\"D\"))\n",
    "\n",
    "\"\"\"\n",
    "Get the ERA5 data.\n",
    "We will rechunk it to a single chunk to make it compatible with other codes on the platform especially bias-correction.\n",
    "We are also taking the daily min and max temperatures as well as the daily total precipitation.\n",
    "\"\"\"\n",
    "# We will add a wrapper to ensure that the following operations will preserve the original data attributes,\n",
    "# such as units and variable names.\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    ERA5_reference = subset.subset_shape(\n",
    "        ds.sel(time=slice(reference_start_day, reference_end_day)), basin_contour\n",
    "    )\n",
    "    ERA5_tmin = ERA5_reference.t2m.resample(time=\"1D\").min().chunk(time=-1)\n",
    "    ERA5_tmax = ERA5_reference.t2m.resample(time=\"1D\").max().chunk(time=-1)\n",
    "    ERA5_pr = ERA5_reference.tp.resample(time=\"1D\").sum().chunk(time=-1)\n",
    "\n",
    "    # Change the units\n",
    "    ERA5_tmin = ERA5_tmin - 273.15  # K to °C\n",
    "    ERA5_tmin.attrs[\"units\"] = \"degC\"\n",
    "\n",
    "    ERA5_tmax = ERA5_tmax - 273.15  # K to °C\n",
    "    ERA5_tmax.attrs[\"units\"] = \"degC\"\n",
    "\n",
    "    ERA5_pr = ERA5_pr * 1000  # m to mm\n",
    "    ERA5_pr.attrs[\"units\"] = \"mm\"\n",
    "\n",
    "    # Average the variables spatially\n",
    "    ERA5_tmin = ERA5_tmin.mean({\"latitude\", \"longitude\"})\n",
    "    ERA5_tmax = ERA5_tmax.mean({\"latitude\", \"longitude\"})\n",
    "    ERA5_pr = ERA5_pr.mean({\"latitude\", \"longitude\"})\n",
    "\n",
    "    # Ensure that the precipitation is non-negative, which can happen with some reanalysis models.\n",
    "    ERA5_pr = np.maximum(ERA5_pr, 0)\n",
    "\n",
    "    # Transform them to a dataset such that they can be written with attributes to netcdf\n",
    "    ERA5_tmin = ERA5_tmin.to_dataset(name=\"tmin\", promote_attrs=True)\n",
    "    ERA5_tmax = ERA5_tmax.to_dataset(name=\"tmax\", promote_attrs=True)\n",
    "    ERA5_pr = ERA5_pr.to_dataset(name=\"pr\", promote_attrs=True)\n",
    "\n",
    "    # Write to disk.\n",
    "    # Here is where we write to disk and where the notebook will fail if running it from the original location on the server (which is read-only).\n",
    "    # Please move the notebooks to your writable-workspace.\n",
    "    ERA5_weather = xr.merge([ERA5_tmin, ERA5_tmax, ERA5_pr])\n",
    "    ERA5_weather.to_netcdf(tmp / \"ERA5_meteo_data.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now also get the climate model data\n",
    "\n",
    "Use the connection to PanGEO to gather the CMIP6 model data for the MIROC6 model. Other models are available, as described in the tutorial Notebook \"08 - Getting and Bias-Correcting CMIP6 data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Climate model to use.\n",
    "climate_model = \"MIROC6\"\n",
    "\n",
    "# Get the catalog info from the PANGEO dataset, which basically is a list of links to the various products.\n",
    "fsCMIP = gcsfs.GCSFileSystem(token=\"anon\", access=\"read_only\")\n",
    "col = intake.open_esm_datastore(\n",
    "    \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    ")\n",
    "\n",
    "# We will add a wrapper to ensure that the following operations will preserve the original data attributes, such as units and variable names.\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    # Load the files from the PanGEO catalogs, for reference and future variables of temperature and precipitation.\n",
    "    out = {}\n",
    "    for exp in [\"historical\", \"ssp585\"]:\n",
    "        if exp == \"historical\":\n",
    "            period_start = reference_start_day\n",
    "            period_end = reference_end_day\n",
    "        else:\n",
    "            period_start = future_start_day\n",
    "            period_end = future_end_day\n",
    "\n",
    "        out[exp] = {}\n",
    "        for variable in [\"tasmin\", \"tasmax\", \"pr\"]:\n",
    "            print(exp, variable)\n",
    "            query = dict(\n",
    "                experiment_id=exp,\n",
    "                table_id=\"day\",\n",
    "                variable_id=variable,\n",
    "                member_id=\"r1i1p1f1\",\n",
    "                source_id=climate_model,\n",
    "            )\n",
    "            col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "            mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "\n",
    "            ds = xr.open_zarr(mapper, consolidated=True).sel(\n",
    "                time=slice(period_start, period_end)\n",
    "            )\n",
    "\n",
    "            if \"height\" in ds.coords:\n",
    "                ds = ds.drop_vars(\"height\")\n",
    "\n",
    "            # Set the date to the midnight of the given day.\n",
    "            ds = ds.assign_coords(time=ds.time.dt.floor(\"D\"))\n",
    "\n",
    "            out[exp][variable] = average.average_shape(\n",
    "                ds,\n",
    "                basin_contour,\n",
    "            )[\n",
    "                variable\n",
    "            ].chunk(-1)\n",
    "\n",
    "\n",
    "# We can now extract the variables that we will need later:\n",
    "historical_tasmax = out[\"historical\"][\"tasmax\"]\n",
    "historical_tasmin = out[\"historical\"][\"tasmin\"]\n",
    "historical_pr = out[\"historical\"][\"pr\"]\n",
    "future_tasmax = out[\"ssp585\"][\"tasmax\"]\n",
    "future_tasmin = out[\"ssp585\"][\"tasmin\"]\n",
    "future_pr = out[\"ssp585\"][\"pr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change units\n",
    "\n",
    "Climate models and reanalysis datasets have often differing units to those expected by Raven. Here we update units to make them compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we need to make sure that our units are all in the correct format.\n",
    "# You can play around with the tools we've seen thus far to explore the units and make sure everything is consistent.\n",
    "\n",
    "# Let's start with precipitation:\n",
    "# The CMIP data is a rate rather than an absolute value, so let's get the absolute values:\n",
    "historical_pr = xclim.core.units.rate2amount(historical_pr)\n",
    "future_pr = xclim.core.units.rate2amount(future_pr)\n",
    "\n",
    "# Now we can actually convert units in absolute terms.\n",
    "historical_pr = xclim.core.units.convert_units_to(historical_pr, \"mm\", context=\"hydro\")\n",
    "future_pr = xclim.core.units.convert_units_to(future_pr, \"mm\", context=\"hydro\")\n",
    "\n",
    "# Now let's do temperature:\n",
    "historical_tasmin = xclim.core.units.convert_units_to(historical_tasmin, \"degC\")\n",
    "historical_tasmax = xclim.core.units.convert_units_to(historical_tasmax, \"degC\")\n",
    "future_tasmin = xclim.core.units.convert_units_to(future_tasmin, \"degC\")\n",
    "future_tasmax = xclim.core.units.convert_units_to(future_tasmax, \"degC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply bias-correction to the climate model data\n",
    "\n",
    "Here is where we perform the bias-correction to the reference and future climate data in order to remove biases as seen between the reference and historical data. The future dataset is then corrected with the same adjustment factors as those in the reference period. Feel free to modify the bias-correction method, quantiles, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xsdba utilities to give information on the type of window used for the bias correction.\n",
    "\n",
    "xsdba.DetrendedQuantileMapping._allow_diff_training_times = True\n",
    "# xsdba.DetrendedQuantileMapping._allow_diff_calendars = True\n",
    "\n",
    "group_month_window = xsdba.utils.Grouper(\"time.dayofyear\", window=15)\n",
    "\n",
    "# This is an adjusting function.\n",
    "# It builds the tool that will perform the corrections.\n",
    "\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_weather.pr,\n",
    "    hist=historical_pr,\n",
    "    nquantiles=50,\n",
    "    kind=\"+\",\n",
    "    group=group_month_window,\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period.\n",
    "corrected_ref_precip = Adjustment.adjust(historical_pr, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period.\n",
    "corrected_fut_precip = Adjustment.adjust(future_pr, interp=\"linear\")\n",
    "\n",
    "# Ensure that the precipitation is non-negative, which can happen with some climate models.\n",
    "corrected_ref_precip = corrected_ref_precip.where(corrected_ref_precip > 0, 0)\n",
    "corrected_fut_precip = corrected_fut_precip.where(corrected_fut_precip > 0, 0)\n",
    "\n",
    "# Train the model to find the correction factors for the maximum temperature (tasmax) data.\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_weather.tmax,\n",
    "    hist=historical_tasmax,\n",
    "    nquantiles=50,\n",
    "    kind=\"+\",\n",
    "    group=group_month_window,\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period.\n",
    "corrected_ref_tasmax = Adjustment.adjust(historical_tasmax, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period.\n",
    "corrected_fut_tasmax = Adjustment.adjust(future_tasmax, interp=\"linear\")\n",
    "\n",
    "# Train the model to find the correction factors for the minimum temperature (tasmin) data.\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_weather.tmin,\n",
    "    hist=historical_tasmin,\n",
    "    nquantiles=50,\n",
    "    kind=\"+\",\n",
    "    group=group_month_window,\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period.\n",
    "corrected_ref_tasmin = Adjustment.adjust(historical_tasmin, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period.\n",
    "corrected_fut_tasmin = Adjustment.adjust(future_tasmin, interp=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the NetCDF files\n",
    "\n",
    "Now that the datasets are created, we can generate files so that Raven can access them. This might take a bit of time since everything up until now has been done in a \"lazy\" framework by Python. Data processing is actually just now really starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the reference corrected data into netCDF file.\n",
    "# We will then apply a special code to remove a dimension in the dataset to make it applicable to the RAVEN models.\n",
    "ref_dataset = xr.merge(\n",
    "    [\n",
    "        corrected_ref_precip.to_dataset(name=\"pr\"),\n",
    "        corrected_ref_tasmax.to_dataset(name=\"tasmax\"),\n",
    "        corrected_ref_tasmin.to_dataset(name=\"tasmin\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write to temporary folder.\n",
    "fn_tmp_ref = tmp / \"reference_dataset_tmp.nc\"\n",
    "ref_dataset.to_netcdf(fn_tmp_ref)\n",
    "\n",
    "# Convert the future corrected data into netCDF file.\n",
    "fut_dataset = xr.merge(\n",
    "    [\n",
    "        corrected_fut_precip.to_dataset(name=\"pr\"),\n",
    "        corrected_fut_tasmax.to_dataset(name=\"tasmax\"),\n",
    "        corrected_fut_tasmin.to_dataset(name=\"tasmin\"),\n",
    "    ]\n",
    ")\n",
    "# Write to temporary folder.\n",
    "with ProgressBar():\n",
    "    fn_tmp_fut = tmp / \"future_dataset_tmp.nc\"\n",
    "    fut_dataset.to_netcdf(fn_tmp_fut)\n",
    "\n",
    "    # Write the data to disk to a temporary location for future use.\n",
    "    ref_dataset = xr.open_dataset(fn_tmp_ref)\n",
    "    ref_dataset.isel(geom=0).squeeze().to_netcdf(tmp / \"reference_dataset.nc\")\n",
    "\n",
    "    fut_dataset = xr.open_dataset(fn_tmp_fut)\n",
    "    fut_dataset.isel(geom=0).squeeze().to_netcdf(tmp / \"future_dataset.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Set-up the hydrological model </span>\n",
    "\n",
    "Now that we have geographic and meteorological input data available, we can setup a Raven hydrological model and calibrate it. Many more details can be found in the documentation and tutorial notebooks.\n",
    "\n",
    "Start by setting up the configuration for the GR4JCN hydrological model we will use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hydrological response unit.\n",
    "# We can use the geographic information we gathered previously to populate the fields for the HRU.\n",
    "hru = {\n",
    "    \"area\": all_properties[\"area\"],\n",
    "    \"elevation\": all_properties[\"elevation\"],\n",
    "    \"latitude\": all_properties[\"latitude\"],\n",
    "    \"longitude\": all_properties[\"longitude\"],\n",
    "    \"hru_type\": \"land\",\n",
    "}\n",
    "\n",
    "# Establish the start date for the calibration.\n",
    "# This is set in the model configuration, so the calibrator will simply execute the model which has been pre-configured to run on this period.\n",
    "start_date = dt.datetime(1981, 1, 1)\n",
    "end_date = dt.datetime(1985, 12, 31)\n",
    "\n",
    "# The data types available in the forcing netcdf file from ERA5, as per the tutorials.\n",
    "data_type = [\"TEMP_MAX\", \"TEMP_MIN\", \"PRECIP\"]\n",
    "\n",
    "# Alternative variable names as described in the tutorial.\n",
    "alt_names = {\n",
    "    \"TEMP_MIN\": \"tmin\",\n",
    "    \"TEMP_MAX\": \"tmax\",\n",
    "    \"PRECIP\": \"pr\",\n",
    "}\n",
    "\n",
    "# The data keywords necessary to indicate the elevation, latitude and longitude of the ERA5 forcing data.\n",
    "# Here we use the information for the basin average as the ERA5 data is averaged on the watershed.\n",
    "data_kwds = {\n",
    "    \"ALL\": {\n",
    "        \"elevation\": hru[\"elevation\"],\n",
    "        \"latitude\": hru[\"latitude\"],\n",
    "        \"longitude\": hru[\"longitude\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Give a name to the simulation.\n",
    "run_name = \"Paper_example_simulation\"\n",
    "\n",
    "# Set up the gauge object that includes meteorological data from ERA5.\n",
    "gauge = [\n",
    "    rc.Gauge.from_nc(\n",
    "        tmp\n",
    "        / \"ERA5_meteo_data.nc\",  # Path to the ERA5 file containing all three meteorological variables\n",
    "        data_type=data_type,  # Note that this is the list of all the variables\n",
    "        alt_names=alt_names,  # Note that all variables here are mapped to their names in the netcdf file.\n",
    "        data_kwds=data_kwds,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Read the streamflow from the HYSETS catchment data for this basin\n",
    "discharge_data = [rc.ObservationData.from_nc(streamflow_file, alt_names=\"discharge\")]\n",
    "\n",
    "# Which evaluation metric do we want to use for calibration.\n",
    "# Raven will return this by default after each run, and the optimizer will read it directly to calibrate.\n",
    "eval_metrics = (\"NASH_SUTCLIFFE\",)\n",
    "\n",
    "# Build the model configuration according to user preferences and inputs\n",
    "model_config = GR4JCN(\n",
    "    ObservationData=discharge_data,\n",
    "    Gauge=gauge,\n",
    "    HRUs=[hru],\n",
    "    StartDate=start_date,\n",
    "    EndDate=end_date,\n",
    "    RunName=run_name,\n",
    "    EvaluationMetrics=eval_metrics,  # We add this code to tell Raven which objective function we want to pass.\n",
    "    SuppressOutput=True,  # This stops Raven from generating the output .nc files at each iteration.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrological model calibration\n",
    "\n",
    "We have finished building the model configuration. We can now focus on the optimizer itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In order to calibrate your model, you need to give the lower and higher bounds of the model.\n",
    "# In this case, we are passing the boundaries for a GR4JCN, but it's important to change them, if you are using another model.\n",
    "low = (0.01, -15.0, 10.0, 0.0, 1.0, 0.0)\n",
    "high = (2.5, 10.0, 700.0, 7.0, 30.0, 1.0)\n",
    "\n",
    "# Random seed.\n",
    "# We will provide one for consistency purposes, but operationally this should not be provided.\n",
    "# FIXME: This will change in numpy v2.0, so we will need to update this code then.\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Build the optimizer object\n",
    "spot_setup = SpotSetup(\n",
    "    config=model_config,\n",
    "    low=low,\n",
    "    high=high,\n",
    ")\n",
    "\n",
    "# Maximum number of model evaluations.\n",
    "# We only use 200 here to keep the computation time as low as possible, but you will want to increase this for operational use, perhaps to 2000-5000 depending on the model.\n",
    "max_iterations = 200\n",
    "\n",
    "# Set up the spotpy sampler with the method, the setup configuration, a run name and other options.\n",
    "# Please refer to the spotpy documentation for more options.\n",
    "# We recommend sticking to this format for efficiency of most applications.\n",
    "# Here we use DDS as the optimization algorithm. More are available: see the Spotpy documentation for more information.\n",
    "# Here, DDS is used as it is powerful and particularly useful for optimizations with small evaluation budgets.\n",
    "# For more details on DDS, see:\n",
    "#\n",
    "# Tolson, B.A. and Shoemaker, C.A., 2007. Dynamically dimensioned search algorithm for computationally efficient watershed model calibration. Water Resources Research, 43(1)\n",
    "sampler = spotpy.algorithms.dds(\n",
    "    spot_setup, dbname=\"RAVEN_model_run\", dbformat=\"ram\", save_sim=False\n",
    ")\n",
    "\n",
    "# Launch the actual optimization.\n",
    "# Multiple trials can be launched, where the entire process is repeated and the best overall value from all trials is returned.\n",
    "sampler.sample(max_iterations, trials=1)\n",
    "\n",
    "# Get the model diagnostics.\n",
    "diag = spot_setup.diagnostics\n",
    "\n",
    "# Get all the values of each iteration.\n",
    "results = sampler.getdata()\n",
    "\n",
    "# Get the raw results directly in an array.\n",
    "bestindex, bestobjfun = spotpy.analyser.get_maxlikeindex(\n",
    "    results\n",
    ")  # Want to get the MAX NSE (change for min for RMSE).\n",
    "best_model_run = list(\n",
    "    results[bestindex][0]\n",
    ")  # Get the parameter set returning the best NSE.\n",
    "optimized_parameters = best_model_run[\n",
    "    1:-1\n",
    "]  # Remove the NSE value (position 0) and the ID at the last position to get the actual parameter set.\n",
    "\n",
    "# Display the parameter set ready to use in a future run:\n",
    "print(optimized_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the calibrated hydrological model on a validation period\n",
    "\n",
    "Now that the hydrological model has been calibrated, we can use these parameters to run the model on an independent period for validation, using ERA5 as the observation weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy the configuration of the previous model that we will modify for our validation:\n",
    "model_validation = model_config.duplicate(\n",
    "    StartDate=dt.datetime(1986, 1, 1),\n",
    "    EndDate=dt.datetime(1990, 12, 31),\n",
    "    SuppressOutput=False,\n",
    ").set_params(optimized_parameters)\n",
    "\n",
    "sim_output = Emulator(config=model_validation).run()\n",
    "\n",
    "# Get validation NSE (note we are counting the first year without warm-up).\n",
    "NSE = sim_output.diagnostics[\"DIAG_NASH_SUTCLIFFE\"]\n",
    "\n",
    "# Plot the model output.\n",
    "sim_output.hydrograph.q_sim.plot(color=\"blue\", label=\"Simulation\")\n",
    "sim_output.hydrograph.q_obs.plot(color=\"black\", label=\"Observation\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation period - NSE=\" + str(NSE[0]))\n",
    "plt.ylabel(\"Streamflow (m³/s)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Climate change impacts on hydrology</span>\n",
    "\n",
    "We can now run GR4JCN to obtain streamflow using the climate model data. We will run the calibrated hydrological model with reference and future data and compare results.\n",
    "\n",
    "### Reference period simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a gauge for Raven to read-in the reference climate data, just like for ERA5.\n",
    "gauge_ref = [\n",
    "    rc.Gauge.from_nc(\n",
    "        tmp\n",
    "        / \"reference_dataset.nc\",  # Path to the CMIP6 model reference data netcdf file.\n",
    "        data_type=data_type,\n",
    "        alt_names=alt_names,\n",
    "        data_kwds=data_kwds,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Copy the configuration of the previous model that we will modify for our simulation on the reference period.\n",
    "model_config_reference = model_validation.duplicate(\n",
    "    Gauge=gauge_ref,\n",
    "    StartDate=reference_start_day\n",
    "    + dt.timedelta(days=1),  # Add a day here to account for the UTC lag in ERA5.\n",
    "    EndDate=reference_end_day,\n",
    ")\n",
    "\n",
    "# Run the model from the configuration and get the outputs.\n",
    "ref_output = Emulator(config=model_config_reference).run()\n",
    "\n",
    "# Plot the model output.\n",
    "# Note that both simulations should have similar hydrological regime but day-to-day variability is not expected to match.\n",
    "ref_output.hydrograph.q_sim.plot(color=\"blue\", label=\"Reference period simulation\")\n",
    "ref_output.hydrograph.q_obs.plot(color=\"black\", label=\"Observation\")\n",
    "plt.legend()\n",
    "plt.title(\"Reference period\")\n",
    "plt.ylabel(\"Streamflow (m³/s)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future period simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a gauge for Raven to read-in the future climate data, just like for the reference data.\n",
    "gauge_fut = [\n",
    "    rc.Gauge.from_nc(\n",
    "        tmp / \"future_dataset.nc\",  # Path to the CMIP6 model reference data netcdf file\n",
    "        data_type=data_type,\n",
    "        alt_names=alt_names,\n",
    "        data_kwds=data_kwds,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Copy the configuration of the previous model that we will modify for our simulation on the reference period.\n",
    "model_config_future = model_validation.duplicate(\n",
    "    Gauge=gauge_fut,\n",
    "    StartDate=future_start_day + dt.timedelta(days=1),\n",
    "    EndDate=future_end_day,\n",
    "    ObservationData=None,  # There are no observations for the future period.\n",
    ")\n",
    "\n",
    "# Run the model and get the outputs and hydrographs.\n",
    "fut_output = Emulator(config=model_config_future).run()\n",
    "\n",
    "# Plot the model output.\n",
    "fut_output.hydrograph.q_sim.plot(color=\"blue\", label=\"Future simulation\")\n",
    "plt.legend()\n",
    "plt.title(\"Future period\")\n",
    "plt.ylabel(\"Streamflow (m³/s)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results\n",
    "We can now compare the results between:\n",
    "- The observed flows;\n",
    "- The simulation flows on the validation period;;\n",
    "- The reference period flows;\n",
    "- The future period flows.\n",
    "\n",
    "Results cannot be compared on a day-to-day basis because climate models do not reflect actual weather data. Therefore, we will compare the mean annual hydrographs to see changes in long-term flow patterns. Note that this test only uses 10 years (5 for the validation period) which is insufficient. Operational tests should include more years (ideally 30 or more) to reflect the climatology of the various periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the mean annual hydrograph for each simulation.\n",
    "observed_flows = ref_output.hydrograph.q_obs.groupby(\"time.dayofyear\").mean()\n",
    "simulated_flows = sim_output.hydrograph.q_obs.groupby(\"time.dayofyear\").mean()\n",
    "reference_flows = ref_output.hydrograph.q_sim.groupby(\"time.dayofyear\").mean()\n",
    "future_flows = fut_output.hydrograph.q_sim.groupby(\"time.dayofyear\").mean()\n",
    "\n",
    "# Plot the model output.\n",
    "observed_flows.plot(color=\"black\", label=\"Observation\", x=\"dayofyear\")\n",
    "simulated_flows.plot(color=\"green\", label=\"Simulation\", x=\"dayofyear\")\n",
    "reference_flows.plot(color=\"blue\", label=\"Reference\", x=\"dayofyear\")\n",
    "future_flows.plot(color=\"red\", label=\"Future\", x=\"dayofyear\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Streamflow (m³/s)\")\n",
    "plt.xlabel(\"Day of year\")\n",
    "plt.xlim([0, 365])\n",
    "plt.title(\"Comparison of mean annual hydrographs\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the simulation and reference period flows are similar to the observations, whereas the future flows show a hastening of the springmelt along with higher winter flows.\n",
    "\n",
    "Hydrographs could also be analysed using the tools shown in the \"Time-series analysis\" notebook.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
