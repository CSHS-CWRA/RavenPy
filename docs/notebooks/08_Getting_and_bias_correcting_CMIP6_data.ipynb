{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Getting and bias-correcting CMIP6 climate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying bias correction on climate model data to perform climate change impact studies on hydrology\n",
    "\n",
    "This notebook will guide you on how to conduct bias correction of climate model outputs that will be fed as inputs to the hydrological model `Raven` to perform climate change impact studies on hydrology.\n",
    "\n",
    "## Geographic data\n",
    "In this tutorial, we will be using the shapefile or GeoJSON file for watershed contours as generated in previous notebooks. The file can be uploaded to your workspace here and used directly in the cells below. In this notebook, we present a quick demonstration of the bias-correction approach on a small and predetermined dataset, but you can use your own basin according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import gcsfs\n",
    "import intake\n",
    "import xarray as xr\n",
    "import xclim\n",
    "import xsdba\n",
    "from clisops.core import average, subset\n",
    "from numba.core.errors import NumbaDeprecationWarning\n",
    "\n",
    "# Utility that simplifies working with test data hosted on GitHub\n",
    "from ravenpy.testing.utils import get_file\n",
    "\n",
    "tmp = Path(tempfile.mkdtemp())\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=NumbaDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to a real catchment and test-case.\n",
    "In this notebook, we will perform bias-correction on a real catchment using real data! You can change the input file for the contours, the catchment properties and other such parameters. The previous notebooks show how to extract basin area, latitude, and longitude, so use those to generate the required information if it is not readily available for your catchment.\n",
    "\n",
    "Let's first start by providing some basic information:\n",
    "\n",
    "- basin_contour: The shapefile or geojson of the watershed boundaries (if it is a shapefile, it has to be a zip-file containing the .shp, .shx and .prj files)\n",
    "- reference_start_day: The start day of the reference period\n",
    "- reference_end_day: The end day of the reference period\n",
    "- future_start_day: The start day of the future period\n",
    "- future_end_day: The end day of the future period\n",
    "- climate_model: The name of the climate model. Must be selected from the available list for this notebook. However, if you want to use other data, the bias-correction step will still be applicable if you follow the same logic and formats as shown here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We get the basin contour for testing on a server.\n",
    "# You can replace the getfile method by a string containing the path to your own geojson.\n",
    "\n",
    "# Get basin contour.\n",
    "basin_contour = get_file(\"notebook_inputs/input.geojson\")\n",
    "\n",
    "reference_start_day = dt.datetime(1980, 12, 31)\n",
    "reference_end_day = dt.datetime(1991, 1, 1)\n",
    "# Notice we are using one day before and one day after the desired period of 1981-01-01 to 1990-12-31.\n",
    "# This is to account for any UTC shifts that might require getting data in a previous or later time.\n",
    "\n",
    "future_start_day = dt.datetime(2080, 12, 31)\n",
    "future_end_day = dt.datetime(2091, 1, 1)\n",
    "# Notice we are using one day before and one day after the desired period of 1981-01-01 to 1990-12-31.\n",
    "# This is to account for any UTC shifts that might require getting data in a previous or later time.\n",
    "\n",
    "\"\"\"\n",
    "Choose a climate model from the list below, which have the daily data required for Raven. Depending on the period required, it is possible that some\n",
    "models will cause errors that need to be addressed specifically using date conversions. In those cases, please select another model or adjust the datetime\n",
    "data to your needs.\n",
    "\n",
    "ACCESS-CM2\n",
    "ACCESS-ESM1-5\n",
    "AWI-CM-1-1-MR\n",
    "BCC-CSM2-MR\n",
    "CESM2-WACCM\n",
    "CMCC-CM2-SR5\n",
    "CMCC-ESM2\n",
    "CanESM5\n",
    "EC-Earth3\n",
    "EC-Earth3-CC\n",
    "EC-Earth3-Veg\n",
    "EC-Earth3-Veg-LR\n",
    "FGOALS-g3\n",
    "GFDL-CM4\n",
    "GFDL-ESM4\n",
    "INM-CM4-8\n",
    "INM-CM5-0\n",
    "IPSL-CM6A-LR\n",
    "KACE-1-0-G\n",
    "KIOST-ESM\n",
    "MIROC6\n",
    "MPI-ESM1-2-HR\n",
    "MPI-ESM1-2-LR\n",
    "MRI-ESM2-0\n",
    "NESM3\n",
    "NorESM2-LM\n",
    "NorESM2-MM\n",
    "\"\"\"\n",
    "\n",
    "climate_model = \"MIROC6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CMIP6 data from the cloud\n",
    "\n",
    "Accessing and downloading climate data can be a painful and time-consuming endeavour. PAVICS-Hydro provides a method to gather data quickly and efficiently, with as little user-input as possible. We use the PanGEO catalog for cloud climate data, and with a few simple keywords, we can automatically extract the required data from the climate model simulations. Furthermore, we can also automatically subset it to our precise location as defined by the watershed boundaries, and also extract only the time period of interest.\n",
    "\n",
    "Let's start by opening the catalog of available data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the filesystem that allows reading data.\n",
    "# Data is read on the Google Cloud Services, which host a copy of the CMIP6 (and other) data.\n",
    "fsCMIP = gcsfs.GCSFileSystem(token=\"anon\", access=\"read_only\")\n",
    "\n",
    "# Get the catalog info from the PANGEO dataset, which basically is a list of links to the various products.\n",
    "col = intake.open_esm_datastore(\n",
    "    \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    ")\n",
    "\n",
    "# Print the contents of the catalog, so we can see the classification system.\n",
    "display(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of climate models (source_id), experiments, members, and other classifications. Let's see the list of available models, for example (source_id):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the list of models.\n",
    "# Replace \"source_id\" with any of the catalog categories (table_id, activity_id, variable_id, etc.)\n",
    "list(col.df.source_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will work with MIROC6, but you can use any other model from the list established previously.\n",
    "\n",
    "Now, we can be more selective about what we want to get from the CMIP6 project data:\n",
    "\n",
    "- source_id: The climate model, in this case 'MIROC6'\n",
    "- experiment_id: The forcing scenario. Here we will use 'historical' (for the historical period) and for future data we could use any of the SSP simulations, such as 'ssp585' or 'ssp245'.\n",
    "- table_id: The timestep of the model simulation. Here we will use 'day' for daily data, but some models have monthly and 3-hourly data, for example.\n",
    "- variable_id: The codename for the variable of interest. Here we will want 'tasmin', 'tasmax', and 'pr' for minimum temperature, maximum temperature and total precipitation, respectively.\n",
    "- member_id: The code identifying the model member. Some models are run multiple times with varying initial conditions to represent natural variability. Here we will only focus on the first member 'r1i1p1f1'.\n",
    "\n",
    "You can find more information about available data on the CMIP6 project webpage and [data nodes] (https://esgf-node.llnl.gov/projects/cmip6/).\n",
    "\n",
    "Let's now see what the PanGEO catalog returns when we ask to filter according to all of these criteria:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a query dictionary for all of our requests, for tasmin.\n",
    "query = dict(\n",
    "    experiment_id=\"historical\",\n",
    "    table_id=\"day\",\n",
    "    variable_id=\"tasmin\",\n",
    "    member_id=\"r1i1p1f1\",\n",
    "    source_id=climate_model,\n",
    ")\n",
    "# Return the filtered list.\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "\n",
    "# Show the filtered list.\n",
    "display(col_subset.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the list contains only one item: The daily tasmin variable, for the historical period of member r1i1p1f1 from the MIROC6 model, as requested! We can also see the path where that file resides on the \"zstore\", which is where it is stored on the Google Cloud service. We can now get the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the object locator object.\n",
    "mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to open the dataset with xarray by using the 'open_zarr()' function. The following block performs multiple operations to get the data that we want:\n",
    "\n",
    "- It opens the data using xarray\n",
    "- It extracts only the times that we need for the reference/historical period\n",
    "- It then subsets it spatially by getting only the points within the catchment boundaries. If your catchments is too small and this fails, try with a larger basin or apply a buffer around your boundaries.\n",
    "- Since we are running a lumped model, it take the spatial average.\n",
    "- It will then remove unnecessary coordinates that could cause problems later ('height', in this case)\n",
    "- It will then rechunk the data into a format that makes it much faster to read and process\n",
    "\n",
    "Finally, we will display the output of this entire process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the CMIP6 data from Google Cloud and read it in memory using xarray.\n",
    "# This is done via \"lazy loading\" and is not actually reading the data in memory yet, but is keeping track of what it will need to get, eventually.\n",
    "ds = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "# Convert to numpy.datetime64 object for compatibility.\n",
    "ds = ds.convert_calendar(\"standard\")\n",
    "\n",
    "# Extract only the dates that we really want.\n",
    "# Again, this is done via lazy loading, and is not actually using memory at this point.\n",
    "ds = ds.sel(time=slice(reference_start_day, reference_end_day))\n",
    "\n",
    "# Set the date to the midnight of the given day.\n",
    "ds = ds.assign_coords(time=ds.time.dt.floor(\"D\"))\n",
    "\n",
    "# Use the clisops subsetting tools to extract the data for the watershed boundaries and take the spatial average\n",
    "ds = average.average_shape(ds, basin_contour)\n",
    "\n",
    "# Correct the coordinates that are unnecessary for our variable\n",
    "ds = ds.reset_coords(\"height\", drop=True)\n",
    "\n",
    "# Rechunk the data so it is much faster to read (single chunk rather than 1 chunk per day)\n",
    "historical_tasmin = ds[\"tasmin\"].chunk(-1)\n",
    "\n",
    "# Show the end result!\n",
    "display(historical_tasmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a single chunk of 10 years of tasmin data, as expected! However, you might also have noticed that there is no metadata, such as units and variable properties left in the data array. We can fix that by wrapping the code in a block that forces xarray to keep the metadata.\n",
    "\n",
    "Also, since we will need to use this block of code for each variable, it might become tedious. Therefore, to simplify the code, we can combine everything into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_and_average(mapper, start, end, geometry):\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        ds = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "        # Convert to numpy.datetime64 object for compatibility.\n",
    "        ds = ds.convert_calendar(\"standard\")\n",
    "\n",
    "        # Set the date to the midnight of the given day.\n",
    "        ds = ds.assign_coords(time=ds.time.dt.floor(\"D\"))\n",
    "\n",
    "        # Compute the average over region.\n",
    "        out = average.average_shape(ds.sel(time=slice(start, end)), geometry)\n",
    "\n",
    "        # Convert geometry variables into attributes.\n",
    "        attrs = {\n",
    "            key: out[key].values.item()\n",
    "            for key in out.coords\n",
    "            if key not in [\"time\", \"time_bnds\", \"lon\", \"lat\"]\n",
    "        }\n",
    "        out = out.isel(geom=0).reset_coords(attrs.keys(), drop=True)\n",
    "        out.attrs.update(attrs)\n",
    "\n",
    "        return out.chunk(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! we have all the information we need. Let's repeat the process for the 3 variables and for the reference and future periods using ssp585. You probably don't have to change anything in this following block of code, but you can taylor it to your needs knowing how everything is built now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will add a wrapper to ensure that the following operations will preserve the original data attributes, such as units and variable names.\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    # Load the files from the PanGEO catalogs, for reference and future variables of temperature and precipitation.\n",
    "    out = {}\n",
    "    for exp in [\"historical\", \"ssp585\"]:\n",
    "        if exp == \"historical\":\n",
    "            period_start = reference_start_day\n",
    "            period_end = reference_end_day\n",
    "        else:\n",
    "            period_start = future_start_day\n",
    "            period_end = future_end_day\n",
    "\n",
    "        out[exp] = {}\n",
    "        for variable in [\"tasmin\", \"tasmax\", \"pr\"]:\n",
    "            print(exp, variable)\n",
    "            query = dict(\n",
    "                experiment_id=exp,\n",
    "                table_id=\"day\",\n",
    "                variable_id=variable,\n",
    "                member_id=\"r1i1p1f1\",\n",
    "                source_id=climate_model,\n",
    "            )\n",
    "            col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "            mapper = fsCMIP.get_mapper(col_subset.df.zstore[0])\n",
    "            ds = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "            out[exp][variable] = extract_and_average(\n",
    "                mapper, period_start, period_end, basin_contour\n",
    "            )[variable]\n",
    "\n",
    "# We can now extract the variables that we will need later:\n",
    "historical_tasmax = out[\"historical\"][\"tasmax\"]\n",
    "historical_tasmin = out[\"historical\"][\"tasmin\"]\n",
    "historical_pr = out[\"historical\"][\"pr\"]\n",
    "future_tasmax = out[\"ssp585\"][\"tasmax\"]\n",
    "future_tasmin = out[\"ssp585\"][\"tasmin\"]\n",
    "future_pr = out[\"ssp585\"][\"pr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reference data to prepare bias correction\n",
    "We have extracted the historical period and future period data from the GCM. Now we need the reference data to use as the baseline for bias-correction. Here we will use ERA5 and we will gather it again, since we can't be sure that the dates we selected in the 3rd notebook are still valid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Regenerate the ERA5 data to be sure. In an operational context, you could combine everything onto one notebook and ensure that the\n",
    "# dates and locations are constant!\n",
    "\n",
    "# Get the ERA5 data from the Wasabi/Amazon S3 server.\n",
    "catalog_name = \"https://raw.githubusercontent.com/hydrocloudservices/catalogs/main/catalogs/atmosphere.yaml\"\n",
    "cat = intake.open_catalog(catalog_name)\n",
    "ds = cat.era5_reanalysis_single_levels.to_dask()\n",
    "\n",
    "\"\"\"\n",
    "Get the ERA5 data. We will rechunk it to a single chunk to make it compatible with other codes on the platform, especially bias-correction.\n",
    "We are also taking the daily min and max temperatures as well as the daily total precipitation.\n",
    "\"\"\"\n",
    "# We will add a wrapper to ensure that the following operations will preserve the original data attributes, such as units and variable names.\n",
    "with xr.set_options(keep_attrs=True):\n",
    "    ERA5_reference = subset.subset_shape(\n",
    "        ds.sel(time=slice(reference_start_day, reference_end_day)), basin_contour\n",
    "    ).mean({\"latitude\", \"longitude\"})\n",
    "    ERA5_tmin = (\n",
    "        ERA5_reference.t2m.resample(time=\"1D\")\n",
    "        .min()\n",
    "        .chunk(\n",
    "            time=-1,\n",
    "        )\n",
    "    )\n",
    "    ERA5_tmax = ERA5_reference.t2m.resample(time=\"1D\").max().chunk(time=-1)\n",
    "    ERA5_pr = ERA5_reference.tp.resample(time=\"1D\").sum().chunk(time=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ERA5_pr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we need to make sure that our units are all in the correct format.\n",
    "# You can play around with the tools we've seen thus far to explore the units and make sure everything is consistent.\n",
    "\n",
    "# Let's start with precipitation:\n",
    "ERA5_pr = xclim.core.units.convert_units_to(ERA5_pr, \"mm\", context=\"hydro\")\n",
    "# The CMIP data is a rate rather than an absolute value, so let's get the absolute values:\n",
    "historical_pr = xclim.core.units.rate2amount(historical_pr)\n",
    "future_pr = xclim.core.units.rate2amount(future_pr)\n",
    "\n",
    "# Now we can actually convert units in absolute terms.\n",
    "historical_pr = xclim.core.units.convert_units_to(historical_pr, \"mm\", context=\"hydro\")\n",
    "future_pr = xclim.core.units.convert_units_to(future_pr, \"mm\", context=\"hydro\")\n",
    "\n",
    "# Now let's do temperature:\n",
    "ERA5_tmin = xclim.core.units.convert_units_to(ERA5_tmin, \"degC\")\n",
    "ERA5_tmax = xclim.core.units.convert_units_to(ERA5_tmax, \"degC\")\n",
    "historical_tasmin = xclim.core.units.convert_units_to(historical_tasmin, \"degC\")\n",
    "historical_tasmax = xclim.core.units.convert_units_to(historical_tasmax, \"degC\")\n",
    "future_tasmin = xclim.core.units.convert_units_to(future_tasmin, \"degC\")\n",
    "future_tasmax = xclim.core.units.convert_units_to(future_tasmax, \"degC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now going to be trained to find correction factors between the reference dataset (observations) and historical dataset (climate model outputs for the same time period). The correction factors obtained are then applied to both reference and future climate outputs to correct them. This step is called the bias correction. In this test-case, we apply a method named `detrended quantile mapping`.\n",
    "\n",
    "Here we use the `xsdba` utilities to bias-correct CMIP6 GCM data using ERA5 reanalysis data as the reference. See `xsdba` documentation for more options! (https://xsdba.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "> **Warning**\n",
    "> This following block of code will take a while to run, and some warning messages will appear during the process (related to longitude wrapping and other information on calendar types). Unless an error message appears, the code should run just fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use xsdba utilities to give information on the type of window used for the bias correction.\n",
    "group_month_window = xsdba.utils.Grouper(\"time.dayofyear\", window=15)\n",
    "\n",
    "# This is an adjusting function. It builds the tool that will perform the corrections.\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_pr, hist=historical_pr, nquantiles=50, kind=\"+\", group=group_month_window\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period.\n",
    "corrected_ref_precip = Adjustment.adjust(historical_pr, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period.\n",
    "corrected_fut_precip = Adjustment.adjust(future_pr, interp=\"linear\")\n",
    "\n",
    "# Ensure that the precipitation is non-negative, which can happen with some climate models.\n",
    "corrected_ref_precip = corrected_ref_precip.where(corrected_ref_precip > 0, 0)\n",
    "corrected_fut_precip = corrected_fut_precip.where(corrected_fut_precip > 0, 0)\n",
    "\n",
    "# Train the model to find the correction factors for the maximum temperature (tasmax) data.\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_tmax,\n",
    "    hist=historical_tasmax,\n",
    "    nquantiles=50,\n",
    "    kind=\"+\",\n",
    "    group=group_month_window,\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period.\n",
    "corrected_ref_tasmax = Adjustment.adjust(historical_tasmax, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period.\n",
    "corrected_fut_tasmax = Adjustment.adjust(future_tasmax, interp=\"linear\")\n",
    "\n",
    "# Train the model to find the correction factors for the minimum temperature (tasmin) data.\n",
    "Adjustment = xsdba.DetrendedQuantileMapping.train(\n",
    "    ref=ERA5_tmin,\n",
    "    hist=historical_tasmin,\n",
    "    nquantiles=50,\n",
    "    kind=\"+\",\n",
    "    group=group_month_window,\n",
    ")\n",
    "\n",
    "# Apply the correction factors on the reference period\n",
    "corrected_ref_tasmin = Adjustment.adjust(historical_tasmin, interp=\"linear\")\n",
    "\n",
    "# Apply the correction factors on the future period\n",
    "corrected_fut_tasmin = Adjustment.adjust(future_tasmin, interp=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corrected reference and future data are then converted to netCDF files. This will take a while to run (perhaps a minute or two), since it will need to write the datasets to disk after having processed everything via lazy loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the reference corrected data into netCDF file.\n",
    "# We will then apply a special code to remove a dimension in the dataset to make it applicable to the RAVEN models.\n",
    "ref_dataset = xr.merge(\n",
    "    [\n",
    "        corrected_ref_precip.to_dataset(name=\"pr\"),\n",
    "        corrected_ref_tasmax.to_dataset(name=\"tasmax\"),\n",
    "        corrected_ref_tasmin.to_dataset(name=\"tasmin\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write to temporary folder.\n",
    "fn_ref = tmp / \"reference_dataset.nc\"\n",
    "ref_dataset.to_netcdf(fn_ref)\n",
    "\n",
    "# Convert the future corrected data into netCDF file.\n",
    "fut_dataset = xr.merge(\n",
    "    [\n",
    "        corrected_fut_precip.to_dataset(name=\"pr\"),\n",
    "        corrected_fut_tasmax.to_dataset(name=\"tasmax\"),\n",
    "        corrected_fut_tasmin.to_dataset(name=\"tasmin\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fn_fut = tmp / \"future_dataset.nc\"\n",
    "fut_dataset.to_netcdf(fn_fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the corrected future precipitation.\n",
    "corrected_fut_precip.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare it to the future precipitation without bias-correction.\n",
    "future_pr.plot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
