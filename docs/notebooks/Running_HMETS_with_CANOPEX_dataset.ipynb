{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running HMETS on the 5797 basins of the extended CANOPEX dataset\n",
    "\n",
    "Here we use birdy's WPS client to launch the HMETS hydrological model on the server and analyze the output. We also prepare and gather data directly from the CANOPEX dataset made available freely for all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:36:56.268818Z",
     "iopub.status.busy": "2021-09-08T20:36:56.268370Z",
     "iopub.status.idle": "2021-09-08T20:37:00.359280Z",
     "shell.execute_reply": "2021-09-08T20:37:00.357631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cookie-cutter template necessary to provide the tools, packages and paths for the project. All notebooks\n",
    "# need this template (or a slightly adjusted one depending on the required packages)\n",
    "import datetime as dt\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spotpy\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from ravenpy.new_config import commands as rc\n",
    "from ravenpy.new_config.emulators import HMETS\n",
    "from ravenpy.utilities.new_config.calibration import SpotSetup\n",
    "from ravenpy.utilities.testdata import get_file\n",
    "\n",
    "# DATA MAIN SOURCE - DAP link to CANOPEX dataset. Can be DAP or direct URL:\n",
    "CANOPEX_DAP = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/ets/Watersheds_5797_cfcompliant.nc\"\n",
    "CANOPEX_URL = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/fileServer/birdhouse/ets/Watersheds_5797_cfcompliant.nc\"\n",
    "\n",
    "# Prefer the DAP link\n",
    "ds = xr.open_dataset(CANOPEX_DAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset:\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:00.369961Z",
     "iopub.status.busy": "2021-09-08T20:37:00.368559Z",
     "iopub.status.idle": "2021-09-08T20:37:00.374515Z",
     "shell.execute_reply": "2021-09-08T20:37:00.373296Z"
    }
   },
   "outputs": [],
   "source": [
    "# We could explore the dataset and find a watershed of interest, but for now, let's pick one at random\n",
    "# from the dataset:\n",
    "watershedID = 5600\n",
    "\n",
    "# And show what it includes:\n",
    "ds = ds.isel({\"watershed\": watershedID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the file to disk to make it more efficient to retrieve:\n",
    "fname = \"/tmp/CANOPEX_extracted.nc\"\n",
    "ds.to_netcdf(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:00.388499Z",
     "iopub.status.busy": "2021-09-08T20:37:00.387053Z",
     "iopub.status.idle": "2021-09-08T20:37:00.419350Z",
     "shell.execute_reply": "2021-09-08T20:37:00.419020Z"
    }
   },
   "outputs": [],
   "source": [
    "# With this info, we can gather some properties from the CANOPEX database. This same database is used for\n",
    "# regionalization, so let's query it there where more information is available:\n",
    "tmp = pd.read_csv(get_file(\"regionalisation_data/gauged_catchment_properties.csv\"))\n",
    "\n",
    "basin_area = tmp[\"area\"][watershedID]\n",
    "basin_latitude = tmp[\"latitude\"][watershedID]\n",
    "basin_longitude = tmp[\"longitude\"][watershedID]\n",
    "basin_elevation = tmp[\"elevation\"][watershedID]\n",
    "basin_name = ds.watershed[watershedID].data\n",
    "\n",
    "print(\"Basin name: \", basin_name)\n",
    "print(\"Latitude: \", basin_latitude, \" Â°N\")\n",
    "print(\"Area: \", basin_area, \" km^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we might have the model and data, but we don't have model parameters! We need to calibrate. This next snippet shows how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:02.449304Z",
     "iopub.status.busy": "2021-09-08T20:37:02.447661Z",
     "iopub.status.idle": "2021-09-08T20:37:06.197246Z",
     "shell.execute_reply": "2021-09-08T20:37:06.196910Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will also calibrate on only a subset of the years for now to keep the computations faster in this notebook.\n",
    "start_calib = dt.datetime(1998, 1, 1)\n",
    "end_calib = dt.datetime(1999, 12, 31)\n",
    "\n",
    "# General parameters depending on the data source. We can find them by exploring the CANOPEX dataset in the\n",
    "# cells above.\n",
    "data_type = [\"TEMP_MAX\", \"TEMP_MIN\", \"PRECIP\"]\n",
    "\n",
    "alt_names = {\n",
    "    \"TEMP_MIN\": \"tasmin\",\n",
    "    \"TEMP_MAX\": \"tasmax\",\n",
    "    \"PRECIP\": \"pr\",\n",
    "}\n",
    "\n",
    "hru = {}\n",
    "hru = dict(\n",
    "    area=basin_area,\n",
    "    elevation=basin_elevation,\n",
    "    latitude=basin_latitude,\n",
    "    longitude=basin_longitude,\n",
    "    hru_type=\"land\",\n",
    ")\n",
    "\n",
    "# Set the evaluation metrics to be calculated by Raven\n",
    "eval_metrics = (\"NASH_SUTCLIFFE\",)\n",
    "\n",
    "model_config = HMETS(\n",
    "    ObservationData=rc.ObservationData.from_nc(\n",
    "        CANOPEX_DAP, alt_names=\"discharge\", station_idx=(watershedID,)\n",
    "    ),\n",
    "    # Setup the gauge using the second method, i.e., using a single file that contains all meteorological inputs. As\n",
    "    # you can see, a single gauge is added, but it contains all the information we need.\n",
    "    Gauge=[\n",
    "        rc.Gauge.from_nc(\n",
    "            fname,\n",
    "            data_type=data_type,  # Note that this is the list of all the variables\n",
    "            alt_names=alt_names,  # Note that all variables here are mapped to their names in the netcdf file.\n",
    "            extra={\n",
    "                \"ALL\": {\n",
    "                    \"elevation\": hru[\"elevation\"],\n",
    "                    \"Latitude\": hru[\"latitude\"],\n",
    "                    \"Longitude\": hru[\"longitude\"],\n",
    "                }\n",
    "            },\n",
    "            station_idx=watershedID,\n",
    "        )\n",
    "    ],\n",
    "    HRUs=[hru],\n",
    "    StartDate=start_date,\n",
    "    EndDate=end_date,\n",
    "    RunName=\"CANOPEX_test\",\n",
    "    EvaluationMetrics=eval_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is setup, we can focus on calibrating the parameters using SpotPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# The model parameters bounds can either be set independently or we can use the defaults.\n",
    "low_params = (\n",
    "    0.3,\n",
    "    0.01,\n",
    "    0.5,\n",
    "    0.15,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    -2.0,\n",
    "    0.01,\n",
    "    0.0,\n",
    "    0.01,\n",
    "    0.005,\n",
    "    -5.0,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    0.0,\n",
    "    0.00001,\n",
    "    0.0,\n",
    "    0.00001,\n",
    "    0.0,\n",
    "    0.0,\n",
    ")\n",
    "high_params = (\n",
    "    20.0,\n",
    "    5.0,\n",
    "    13.0,\n",
    "    1.5,\n",
    "    20.0,\n",
    "    20.0,\n",
    "    3.0,\n",
    "    0.2,\n",
    "    0.1,\n",
    "    0.3,\n",
    "    0.1,\n",
    "    2.0,\n",
    "    5.0,\n",
    "    1.0,\n",
    "    3.0,\n",
    "    1.0,\n",
    "    0.02,\n",
    "    0.1,\n",
    "    0.01,\n",
    "    0.5,\n",
    "    2.0,\n",
    ")\n",
    "\n",
    "# Setup the spotpy optimizer\n",
    "spot_setup = SpotSetup(\n",
    "    config=model_config,\n",
    "    low=low_params,\n",
    "    high=high_params,\n",
    "    path=\"/tmp/CANOPEX_NB_test3/\",\n",
    ")\n",
    "# TODO: Allow overwrite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll definitely want to adjust the random seed and number of model evaluations:\n",
    "model_evaluations = (\n",
    "    50  # This is to keep computing time fast for the demo, increase as necessary\n",
    ")\n",
    "\n",
    "# Setup the spotpy sampler with the method, the setup configuration, a run name and other options. Please refer to\n",
    "# the spotpy documentation for more options. We recommend sticking to this format for efficiency of most applications.\n",
    "sampler = spotpy.algorithms.dds(\n",
    "    spot_setup,\n",
    "    dbname=\"CANOPEX_test\",\n",
    "    dbformat=\"ram\",\n",
    "    save_sim=False,\n",
    ")\n",
    "\n",
    "# Launch the actual optimization. Multiple trials can be launched, where the entire process is repeated and\n",
    "# the best overall value from all trials is returned.\n",
    "sampler.sample(model_evaluations, trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:06.200767Z",
     "iopub.status.busy": "2021-09-08T20:37:06.200208Z",
     "iopub.status.idle": "2021-09-08T20:37:06.202836Z",
     "shell.execute_reply": "2021-09-08T20:37:06.202446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the model diagnostics\n",
    "diag = spot_setup.diagnostics\n",
    "\n",
    "# Print the NSE and the parameter set in 2 different ways:\n",
    "print(\"Nash-Sutcliffe value is: \" + str(diag[\"DIAG_NASH_SUTCLIFFE\"]))\n",
    "\n",
    "# Get all the values of each iteration\n",
    "results = sampler.getdata()\n",
    "\n",
    "# Get the raw resutlts directly in an array\n",
    "spotpy.analyser.get_best_parameterset(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:06.205950Z",
     "iopub.status.busy": "2021-09-08T20:37:06.205506Z",
     "iopub.status.idle": "2021-09-08T20:37:06.209274Z",
     "shell.execute_reply": "2021-09-08T20:37:06.208949Z"
    }
   },
   "outputs": [],
   "source": [
    "# And also the NSE value:\n",
    "print(diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we have calibrated the model on the observations for the desired dates. Now, let's run the model on a longer time period and look at the hydrograph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we requested output objects, we can simply access the output objects. The diagnostics is just a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.185154Z",
     "iopub.status.busy": "2021-09-08T20:37:08.184807Z",
     "iopub.status.idle": "2021-09-08T20:37:08.186392Z",
     "shell.execute_reply": "2021-09-08T20:37:08.186728Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can analyze and plot the data directly here to see what it looks like, or we could download the data directly by\n",
    "# changing the asobj=True to asobj=False in the cell above this one.\n",
    "print(diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.189079Z",
     "iopub.status.busy": "2021-09-08T20:37:08.188737Z",
     "iopub.status.idle": "2021-09-08T20:37:08.190806Z",
     "shell.execute_reply": "2021-09-08T20:37:08.190476Z"
    }
   },
   "outputs": [],
   "source": [
    "print(diagnostics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hydrograph` and `storage` outputs are netCDF files storing the time series. These files are opened by default using `xarray`, which provides convenient and powerful time series analysis and plotting tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.194893Z",
     "iopub.status.busy": "2021-09-08T20:37:08.194524Z",
     "iopub.status.idle": "2021-09-08T20:37:08.235636Z",
     "shell.execute_reply": "2021-09-08T20:37:08.235272Z"
    }
   },
   "outputs": [],
   "source": [
    "hydrograph.q_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.238599Z",
     "iopub.status.busy": "2021-09-08T20:37:08.238201Z",
     "iopub.status.idle": "2021-09-08T20:37:08.361328Z",
     "shell.execute_reply": "2021-09-08T20:37:08.361914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the simulated hydrograph\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "hydrograph.q_sim.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.367532Z",
     "iopub.status.busy": "2021-09-08T20:37:08.366839Z",
     "iopub.status.idle": "2021-09-08T20:37:08.375481Z",
     "shell.execute_reply": "2021-09-08T20:37:08.375119Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also get statistics from the data directly here.\n",
    "print(\"Max: \", hydrograph.q_sim.max())\n",
    "print(\"Mean: \", hydrograph.q_sim.mean())\n",
    "print(\n",
    "    \"Monthly means: \",\n",
    "    hydrograph.q_sim.groupby(hydrograph.time.dt.month).mean(dim=\"time\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example of how to download the data directly to analyze locally on your own computer/server, see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-08T20:37:08.380556Z",
     "iopub.status.busy": "2021-09-08T20:37:08.380163Z",
     "iopub.status.idle": "2021-09-08T20:37:08.382445Z",
     "shell.execute_reply": "2021-09-08T20:37:08.382071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rerun the analysis of the WPS response, this type by using asobj=False.\n",
    "[hydrograph, storage, solution, diagnostics, rv] = resp.get(asobj=False)\n",
    "print(hydrograph)\n",
    "print(storage)\n",
    "print(solution)\n",
    "print(diagnostics)\n",
    "print(rv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
